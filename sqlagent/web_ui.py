"""
åŸºäº Streamlit çš„ SQL Agent Web ç•Œé¢
æä¾›èŠå¤©å¼äº¤äº’å’Œç»“æœå¯è§†åŒ–
"""
import streamlit as st
import sys
import os
from io import BytesIO
from datetime import datetime
from typing import Any, Dict, List, Optional
from langchain_core.callbacks import BaseCallbackHandler
import time
import pandas as pd
from sqlalchemy import create_engine
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlagent import SQLAgent, Config

# EChartsï¼ˆå¯é€‰ä¾èµ–ï¼Œæœªå®‰è£…æ—¶è‡ªåŠ¨é™çº§ä¸æ˜¾ç¤ºå›¾è¡¨ï¼‰
try:
    from streamlit_echarts import st_echarts  # type: ignore
    HAS_ECHARTS = True
except Exception:
    HAS_ECHARTS = False


class StreamlitStatusTraceHandler(BaseCallbackHandler):
    """æŠŠå·¥å…·è°ƒç”¨è¿‡ç¨‹å®æ—¶å†™åˆ° Streamlit çš„ st.statusï¼ˆé»˜è®¤æŠ˜å  + è¿è¡ŒçŠ¶æ€ï¼‰ã€‚"""

    def __init__(self, status_box):
        self.status_box = status_box
        self.step_no = 0
        self._current_tool: Optional[str] = None

    @staticmethod
    def _norm(v: Any) -> str:
        if v is None:
            return ""
        if isinstance(v, str):
            return v.strip()
        if isinstance(v, dict):
            # å¸¸è§ï¼š{"query": "..."} / {"tool_input": "..."}
            if "query" in v and isinstance(v["query"], str):
                return v["query"].strip()
            if "tool_input" in v and isinstance(v["tool_input"], str):
                return v["tool_input"].strip()
            return str(v)
        return str(v).strip()

    def on_tool_start(self, serialized: Dict[str, Any], input_str: Any = None, **kwargs) -> None:
        tool_name = (serialized or {}).get("name") or kwargs.get("name") or ""
        normalized_input = self._norm(input_str if input_str is not None else kwargs.get("input"))

        self.step_no += 1
        self._current_tool = tool_name
        self.status_box.write(f"**{self.step_no}. è°ƒç”¨å·¥å…·ï¼š`{tool_name}`**")
        if normalized_input:
            if tool_name == "sql_db_query":
                self.status_box.write("è¾“å…¥ï¼ˆSQLï¼‰ï¼š")
                self.status_box.code(normalized_input, language="sql")
            else:
                self.status_box.write("è¾“å…¥ï¼š")
                self.status_box.code(normalized_input)

    def on_tool_end(self, output: Any, **kwargs) -> None:
        normalized_output = self._norm(output)
        if normalized_output:
            self.status_box.write("è¾“å‡ºï¼š")
            self.status_box.code(normalized_output)

    def on_tool_error(self, error: Exception, **kwargs) -> None:
        self.status_box.write(f"âŒ å·¥å…·æ‰§è¡Œå‡ºé”™ï¼š{error}")


class StreamlitAnswerStreamHandler(BaseCallbackHandler):
    """åªç”¨äºâ€œæœ€ç»ˆå›ç­”â€çš„ token æµå¼å±•ç¤ºã€‚"""

    def __init__(self, placeholder: "st.delta_generator.DeltaGenerator"):
        self.placeholder = placeholder
        self._buf: List[str] = []
        self._last_flush = 0.0

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self._buf.append(token)
        now = time.monotonic()
        # è½»å¾®èŠ‚æµï¼Œé¿å…æ¯ä¸ª token éƒ½è§¦å‘ UI é‡ç»˜
        if now - self._last_flush >= 0.05:
            self.placeholder.markdown("".join(self._buf))
            self._last_flush = now

    def flush(self) -> str:
        text = "".join(self._buf)
        self.placeholder.markdown(text)
        return text


@st.cache_resource
def get_sqlalchemy_engine(db_name: str):
    """å¤ç”¨ SQLAlchemy Engineï¼ˆé¿å…æ¯æ¬¡éƒ½æ–°å»ºè¿æ¥æ± ï¼‰ã€‚"""
    uri = Config.get_db_uri(db_name)
    engine_args = {
        "pool_pre_ping": True,
        "pool_size": 5,
        "max_overflow": 10,
        "pool_recycle": 3600,
        "connect_args": {"connect_timeout": 10},
    }
    return create_engine(uri, **engine_args)




def build_excel_bytes(df: pd.DataFrame) -> bytes:
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="result")
    return buf.getvalue()

def df_preview_text(df: pd.DataFrame, n: int = 20) -> str:
    """ç»™æ¨¡å‹ç”¨çš„æ ·ä¾‹æ–‡æœ¬ï¼šæœ€å¤š n è¡Œï¼Œé¿å…ä¸Šä¸‹æ–‡çˆ†ç‚¸ã€‚"""
    if df is None or df.empty:
        return "(ç©ºç»“æœ)"
    d = df.head(n).copy()
    # å…¨éƒ¨è½¬æˆå­—ç¬¦ä¸²ï¼Œé¿å… datetime/decimal ç­‰åœ¨ markdown é‡Œè¿‡é•¿
    for c in d.columns:
        d[c] = d[c].astype(str)
    try:
        return d.to_markdown(index=False)
    except Exception:
        # å…œåº•ï¼šCSV
        return d.to_csv(index=False)

@st.cache_data(ttl=300, show_spinner=False)
def get_df_for_sql(db_name: str, sql: str) -> pd.DataFrame:
    """ä¸ºå†å²æ¶ˆæ¯é‡ç»˜/ä¸‹è½½é‡è·‘æä¾› DataFrameï¼ˆå¸¦ç¼“å­˜ï¼Œé¿å…é¢‘ç¹æ‰“åº“ï¼‰ã€‚"""
    engine = get_sqlalchemy_engine(db_name)
    return execute_sql_to_df(sql, engine)

def stable_key_for_sql(db_name: str, sql: str) -> str:
    raw = (db_name + "\n" + (sql or "")).encode("utf-8", errors="ignore")
    return hashlib.md5(raw).hexdigest()


def execute_sql_to_df(sql: str, engine) -> pd.DataFrame:
    """
    ä½¿ç”¨ SQLAlchemy çš„ raw_connection è·å– DBAPI è¿æ¥ç›´æ¥æ‰§è¡Œ SQLï¼Œ
    è§„é¿æŸäº›é©±åŠ¨åœ¨åŒ…å« LIKE '%xx%' æ—¶æŠŠ % è¯¯å½“ä½œå ä½ç¬¦å¯¼è‡´çš„æ ¼å¼åŒ–é”™è¯¯ã€‚
    """
    q = (sql or "").strip().rstrip(";")
    if not q:
        return pd.DataFrame()

    conn = engine.raw_connection()
    try:
        cur = conn.cursor()
        cur.execute(q)  # ä¸ä¼ å‚æ•°ï¼Œç¡®ä¿ % ä½œä¸º SQL å­—é¢é‡ç”Ÿæ•ˆ
        rows = cur.fetchall()
        cols = [d[0] for d in (cur.description or [])]
        return pd.DataFrame(list(rows), columns=cols)
    finally:
        try:
            conn.close()
        except Exception:
            pass


def auto_echarts_option(df: pd.DataFrame) -> Optional[Dict[str, Any]]:
    """æ ¹æ® df çš„åˆ—ç±»å‹è‡ªåŠ¨é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ ECharts optionï¼ˆç®€å•è§„åˆ™ç‰ˆï¼‰ã€‚"""
    if df is None or df.empty:
        return None

    # ç»˜å›¾æœ€å¤šä½¿ç”¨ 20 è¡Œï¼ˆä¸ç»“æœé™åˆ¶ä¸€è‡´ï¼‰
    d = df.copy().head(20)

    # å°è¯•è¯†åˆ«æ—¶é—´åˆ—
    datetime_cols: List[str] = []
    for c in d.columns:
        if pd.api.types.is_datetime64_any_dtype(d[c]):
            datetime_cols.append(c)
            continue
        if pd.api.types.is_object_dtype(d[c]):
            # å°è¯• parse
            parsed = pd.to_datetime(d[c], errors="coerce", utc=False)
            if parsed.notna().mean() > 0.8:
                d[c] = parsed
                datetime_cols.append(c)

    numeric_cols = [c for c in d.columns if pd.api.types.is_numeric_dtype(d[c])]
    cat_cols = [c for c in d.columns if c not in numeric_cols and c not in datetime_cols]

    # æ—¶é—´ + æ•°å€¼ => æŠ˜çº¿
    if datetime_cols and numeric_cols:
        x = datetime_cols[0]
        y = numeric_cols[0]
        dd = d[[x, y]].dropna().sort_values(x)
        return {
            "tooltip": {"trigger": "axis"},
            "xAxis": {"type": "category", "data": dd[x].dt.strftime("%Y-%m-%d %H:%M:%S").tolist()},
            "yAxis": {"type": "value"},
            "series": [{"type": "line", "data": dd[y].tolist(), "smooth": True}],
        }

    # åˆ†ç±» + æ•°å€¼ => æ¡å½¢
    if cat_cols and numeric_cols:
        x = cat_cols[0]
        y = numeric_cols[0]
        dd = d[[x, y]].dropna()
        # å–å‰ 20 ç±»
        dd = dd.head(20)
        return {
            "tooltip": {"trigger": "axis"},
            "xAxis": {"type": "category", "data": dd[x].astype(str).tolist(), "axisLabel": {"rotate": 30}},
            "yAxis": {"type": "value"},
            "series": [{"type": "bar", "data": dd[y].tolist()}],
        }

    # ä¸¤ä¸ªæ•°å€¼ => æ•£ç‚¹
    if len(numeric_cols) >= 2:
        x, y = numeric_cols[0], numeric_cols[1]
        dd = d[[x, y]].dropna().head(500)
        return {
            "tooltip": {"trigger": "item"},
            "xAxis": {"type": "value", "name": x},
            "yAxis": {"type": "value", "name": y},
            "series": [{"type": "scatter", "data": dd.values.tolist()}],
        }

    return None

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="SQL Agent - æ™ºèƒ½æŸ¥è¯¢åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ä½¿ç”¨ @st.cache_resource ç¼“å­˜ Agent å¯¹è±¡ï¼ˆè·¨ä¼šè¯å…±äº«ï¼Œåˆ·æ–°é¡µé¢ä¸é‡æ–°åˆå§‹åŒ–ï¼‰
@st.cache_resource
def get_sql_agent(db_name: str = None):
    """
    è·å–ç¼“å­˜çš„ SQL Agent å¯¹è±¡
    ä½¿ç”¨ @st.cache_resource ä½¿è¿æ¥åœ¨æ‰€æœ‰ç”¨æˆ·ä¼šè¯é—´å…±äº«
    åˆ·æ–°é¡µé¢æˆ–æ–°æ ‡ç­¾é¡µéƒ½ä¸ä¼šé‡æ–°åˆå§‹åŒ–
    """
    return SQLAgent(db_name=db_name)

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "db_name" not in st.session_state:
    st.session_state.db_name = Config.DB_NAME

# è·å–ç¼“å­˜çš„ Agentï¼ˆé¦–æ¬¡åŠ è½½ä¼šæ˜¾ç¤ºåŠ è½½æç¤ºï¼‰
try:
    with st.spinner("ğŸ”„ æ­£åœ¨è¿æ¥äº‘ç«¯æ•°æ®åº“å¹¶åˆå§‹åŒ–Agent..."):
        agent = get_sql_agent(st.session_state.db_name)
except Exception as e:
    st.error(f"åˆå§‹åŒ– Agent å¤±è´¥: {e}")
    st.stop()

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("âš™ï¸ é…ç½®")
    
    # æ•°æ®åº“é€‰æ‹©ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æŸ¥è¯¢ï¼‰
    @st.cache_data(ttl=300)  # ç¼“å­˜5åˆ†é’Ÿ
    def get_databases():
        return Config.get_available_databases()
    
    try:
        databases = get_databases()
        selected_db = st.selectbox(
            "é€‰æ‹©æ•°æ®åº“",
            databases,
            index=databases.index(st.session_state.db_name) if st.session_state.db_name in databases else 0
        )
        
        if selected_db != st.session_state.db_name:
            with st.spinner(f"åˆ‡æ¢åˆ°æ•°æ®åº“ {selected_db}..."):
                # æ¸…é™¤ç¼“å­˜ï¼Œé‡æ–°è·å–æ–°æ•°æ®åº“çš„ Agent
                get_sql_agent.clear()
                st.session_state.db_name = selected_db
                st.rerun()  # é‡æ–°è¿è¡Œä»¥ä½¿ç”¨æ–°çš„æ•°æ®åº“
    except Exception as e:
        st.error(f"è·å–æ•°æ®åº“åˆ—è¡¨å¤±è´¥: {e}")
    
    st.divider()
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®åº“ä¿¡æ¯
    st.subheader("ğŸ“Š æ•°æ®åº“ä¿¡æ¯")
    if st.button("æŸ¥çœ‹ Schema"):
        schema_info = agent.get_schema_info()
        st.write(f"**æ•°æ®åº“**: {schema_info['database']}")
        st.write(f"**è¡¨åˆ—è¡¨**: {', '.join(schema_info['tables'])}")
        with st.expander("è¯¦ç»†ç»“æ„"):
            st.code(schema_info['table_info'], language="sql")
    
    st.divider()
    
    # æ¸…ç©ºå¯¹è¯
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

# ä¸»ç•Œé¢
st.title("ğŸ¤– SQL Agent - æ™ºèƒ½ MySQL æŸ¥è¯¢åŠ©æ‰‹")
st.caption(f"å½“å‰æ•°æ®åº“: **{st.session_state.db_name}**")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰SQLï¼Œå…ˆæ˜¾ç¤ºSQL
        if message["role"] == "assistant" and "sql" in message:
            with st.expander("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„ SQL è¯­å¥", expanded=False):
                st.code(message["sql"], language="sql")

        # å¦‚æœå†å²æ¶ˆæ¯é‡Œæœ‰ last_sqlï¼Œåˆ™é‡ç»˜â€œè¡¨æ ¼ + ä¸‹è½½ + å›¾è¡¨â€
        if message["role"] == "assistant" and message.get("last_sql"):
            try:
                df_hist = get_df_for_sql(st.session_state.db_name, message["last_sql"])
                effective_limit = int(message.get("effective_limit") or 20)
                PREVIEW_ROWS = min(effective_limit, 20)
                preview_df = df_hist.head(PREVIEW_ROWS)
                if len(df_hist) <= PREVIEW_ROWS:
                    st.markdown(f"**ğŸ“„ æŸ¥è¯¢ç»“æœï¼ˆå…± {len(df_hist)} è¡Œï¼Œå·²å…¨éƒ¨å±•ç¤ºï¼‰**")
                else:
                    st.markdown(f"**ğŸ“„ æŸ¥è¯¢ç»“æœï¼ˆå‰ {PREVIEW_ROWS} è¡Œ / å…± {len(df_hist)} è¡Œï¼‰**")
                st.dataframe(preview_df, width="stretch")

                # å…¨é‡ä¸‹è½½ï¼ˆExcelï¼‰â€” éœ€è¦å”¯ä¸€ keyï¼Œé¿å… rerun åç»„ä»¶çŠ¶æ€é”™ä¹±
                excel_bytes = build_excel_bytes(df_hist)
                filename = f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                dl_key = f"download-{stable_key_for_sql(st.session_state.db_name, message['last_sql'])}"
                st.download_button(
                    label="â¬‡ï¸ ä¸‹è½½å…¨é‡ç»“æœï¼ˆExcelï¼‰",
                    data=excel_bytes,
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=dl_key,
                )

                if HAS_ECHARTS:
                    option = auto_echarts_option(df_hist)
                    if option:
                        st.markdown("**ğŸ“Š å¯è§†åŒ–ï¼ˆEChartsï¼‰**")
                        st_echarts(option, height="420px", key=f"chart-{dl_key}")
            except Exception as e:
                st.caption(f"âš ï¸ æŸ¥è¯¢ç»“æœå±•ç¤ºå¤±è´¥ï¼š{e}")
        
        # æ˜¾ç¤ºæ¶ˆæ¯å†…å®¹
        st.markdown(message["content"])

# è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    with st.chat_message("assistant"):
        # é»˜è®¤æŠ˜å ï¼Œä½†ä¼šæ˜¾ç¤ºâ€œè¿è¡Œä¸­â€çš„çŠ¶æ€å—ï¼ˆç”¨æˆ·å¯ç‚¹å¼€çœ‹è¿‡ç¨‹ï¼‰
        status_box = st.status("æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“â€¦", expanded=False, state="running")
        live_handler = StreamlitStatusTraceHandler(status_box)

        # ç¬¬1é˜¶æ®µï¼šè¿è¡Œå·¥å…·/SQLï¼ˆä¸æµå¼ï¼‰
        tool_result = agent.run_tools(prompt, callbacks=[live_handler])

        # æŸ¥è¯¢ç»“æŸï¼šæ›´æ–°çŠ¶æ€ï¼ˆä»ç„¶ä¿æŒæŠ˜å ï¼Œé¿å…å é¡µé¢ï¼‰
        if tool_result.get("success"):
            status_box.update(label="æ•°æ®åº“æŸ¥è¯¢å®Œæˆ", state="complete", expanded=False)
        else:
            status_box.update(label="æ•°æ®åº“æŸ¥è¯¢å¤±è´¥", state="error", expanded=False)

        if tool_result.get("success"):
            # å¦‚æœæœ‰ç”Ÿæˆçš„SQLï¼Œå…ˆåœ¨å¯æŠ˜å æ¡†ä¸­å±•ç¤ºï¼ˆåœ¨æœ€ç»ˆå›ç­”ä¹‹å‰ï¼‰
            if tool_result.get("sql"):
                with st.expander("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„ SQL è¯­å¥", expanded=False):
                    st.code(tool_result["sql"], language="sql")

            # SQL ä¸‹æ–¹å±•ç¤ºæ•°æ®ï¼ˆå‰10è¡Œï¼‰+ å…¨é‡ä¸‹è½½ + ECharts
            last_sql = tool_result.get("last_sql", "") or ""
            if last_sql:
                try:
                    engine = get_sqlalchemy_engine(st.session_state.db_name)
                    df = execute_sql_to_df(last_sql, engine)

                    effective_limit = int(tool_result.get("effective_limit") or 20)
                    PREVIEW_ROWS = min(effective_limit, 20)
                    preview_df = df.head(PREVIEW_ROWS)
                    if len(df) <= PREVIEW_ROWS:
                        st.markdown(f"**ğŸ“„ æŸ¥è¯¢ç»“æœï¼ˆå…± {len(df)} è¡Œï¼Œå·²å…¨éƒ¨å±•ç¤ºï¼‰**")
                    else:
                        st.markdown(f"**ğŸ“„ æŸ¥è¯¢ç»“æœï¼ˆå‰ {PREVIEW_ROWS} è¡Œ / å…± {len(df)} è¡Œï¼‰**")
                    # Streamlit æ–°ç‰ˆæ¨èç”¨ width="stretch"
                    st.dataframe(preview_df, width="stretch")

                    # å…¨é‡ä¸‹è½½ï¼ˆExcelï¼‰
                    excel_bytes = build_excel_bytes(df)
                    filename = f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½å…¨é‡ç»“æœï¼ˆExcelï¼‰",
                        data=excel_bytes,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )

                    # ECharts å¯è§†åŒ–
                    if HAS_ECHARTS:
                        option = auto_echarts_option(df)
                        if option:
                            st.markdown("**ğŸ“Š å¯è§†åŒ–ï¼ˆEChartsï¼‰**")
                            st_echarts(option, height="420px")
                        else:
                            st.caption("ğŸ“Š å½“å‰ç»“æœä¸é€‚åˆè‡ªåŠ¨ç»˜å›¾ï¼ˆåˆ—ç±»å‹ä¸è¶³æˆ–æ•°æ®ä¸ºç©ºï¼‰ã€‚")
                    else:
                        st.caption("ğŸ“Š æœªå®‰è£… `streamlit-echarts`ï¼Œæš‚ä¸å±•ç¤ºå›¾è¡¨ã€‚")
                except Exception as e:
                    st.caption(f"âš ï¸ æŸ¥è¯¢ç»“æœå±•ç¤ºå¤±è´¥ï¼š{e}")
            else:
                st.caption("âš ï¸ æœªæ•è·åˆ°å¯ç”¨äºå±•ç¤ºçš„æ•°æ®æŸ¥è¯¢ SQLï¼ˆlast_sql ä¸ºç©ºï¼‰ã€‚")

            # ä¸å†è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆâ€œæœ€ç»ˆæ€»ç»“å›å¤â€ï¼Œæ”¹ä¸ºå›ºå®šæ¨¡æ¿ï¼ˆé›¶é¢å¤– tokenï¼‰
            total_rows = int(df.shape[0]) if "df" in locals() and isinstance(df, pd.DataFrame) else None
            if total_rows is not None:
                final_answer = f"æŸ¥è¯¢å®Œæˆï¼Œå…± {total_rows} è¡Œç»“æœã€‚æ˜ç»†è¯·æŸ¥çœ‹ä¸Šæ–¹è¡¨æ ¼ï¼Œæˆ–ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸‹è½½å…¨é‡ Excelã€‚"
            else:
                final_answer = "æŸ¥è¯¢å®Œæˆã€‚æ˜ç»†è¯·æŸ¥çœ‹ä¸Šæ–¹è¡¨æ ¼ï¼Œæˆ–ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸‹è½½å…¨é‡ Excelã€‚"
            st.markdown(final_answer)

            # ä¿å­˜åˆ°æ¶ˆæ¯å†å²ï¼ˆç”¨äºåˆ·æ–°åä»å¯è§ï¼‰
            message_data = {
                "role": "assistant",
                "content": final_answer
            }
            # å¦‚æœæœ‰SQLï¼Œä¹Ÿä¿å­˜åˆ°æ¶ˆæ¯å†å²ä¸­
            if tool_result.get("sql"):
                message_data["sql"] = tool_result["sql"]
            # ä¿å­˜ last_sqlï¼Œç”¨äº rerun åé‡ç»˜è¡¨æ ¼/ä¸‹è½½/å›¾è¡¨
            if tool_result.get("last_sql"):
                message_data["last_sql"] = tool_result["last_sql"]
            message_data["effective_limit"] = int(tool_result.get("effective_limit") or 20)
            
            st.session_state.messages.append(message_data)
        else:
            error_msg = f"âŒ æŸ¥è¯¢å¤±è´¥: {tool_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            st.error(error_msg)
            st.session_state.messages.append({
                "role": "assistant",
                "content": error_msg
            })

# ï¼ˆå·²ç§»é™¤ï¼‰ç¤ºä¾‹é—®é¢˜ã€ç³»ç»Ÿä¿¡æ¯ï¼šä¿æŒèŠå¤©ç•Œé¢ç®€æ´
